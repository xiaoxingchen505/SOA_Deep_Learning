{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 解析cifar-10数据集\n",
    "运行F_ResNet/src/01_parse_cifar10_to_png.py，将cifar-10数据集解析为png格式   \n",
    "\n",
    "数据存放位置：   \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar-10-python.tar   \n",
    "解压得到：  \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar-10-batches-py\n",
    "\n",
    "经过01_parse_cifar10_to_png.py，得到：  \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar10_train   \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar10_test  \n",
    "\n",
    "\n",
    "### 数据展示\n",
    "<img src=\"imgs/cifar10.png\" width=\"700\" heith=\"700\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     57
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "os.environ['NLS_LANG'] = 'SIMPLIFIED CHINESE_CHINA.UTF8'\n",
    "import sys\n",
    "sys.path.append(BASE_DIR)\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tools.cifar10_dataset import CifarDataset\n",
    "from tools.resnet import resnet56, resnet20\n",
    "from tools.common_tools import ModelTrainer, show_confMat, plot_line\n",
    "\n",
    "class CifarDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        assert (os.path.exists(data_dir)), \"data_dir:{} 不存在！\".format(data_dir)\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self._get_img_info()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn, label = self.img_info[index]\n",
    "        img = Image.open(fn).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.img_info) == 0:\n",
    "            raise Exception(\"未获取任何图片路径，请检查dataset及文件路径！\")\n",
    "        return len(self.img_info)\n",
    "\n",
    "    def _get_img_info(self):\n",
    "        sub_dir_ = [name for name in os.listdir(self.data_dir) if os.path.isdir(os.path.join(self.data_dir, name))]\n",
    "        sub_dir = [os.path.join(self.data_dir, c) for c in sub_dir_]\n",
    "\n",
    "        self.img_info = []\n",
    "        for c_dir in sub_dir:\n",
    "            path_img = [(os.path.join(c_dir, i), int(os.path.basename(c_dir))) for i in os.listdir(c_dir) if\n",
    "                        i.endswith(\"png\")]\n",
    "            self.img_info.extend(path_img)\n",
    "            \n",
    "            \n",
    "def transform_invert(img_, transform_train):\n",
    "    \"\"\"\n",
    "    将data 进行反transfrom操作\n",
    "    :param img_: tensor\n",
    "    :param transform_train: torchvision.transforms\n",
    "    :return: PIL image\n",
    "    \"\"\"\n",
    "    if 'Normalize' in str(transform_train):\n",
    "        norm_transform = list(filter(lambda x: isinstance(x, transforms.Normalize), transform_train.transforms))\n",
    "        mean = torch.tensor(norm_transform[0].mean, dtype=img_.dtype, device=img_.device)\n",
    "        std = torch.tensor(norm_transform[0].std, dtype=img_.dtype, device=img_.device)\n",
    "        img_.mul_(std[:, None, None]).add_(mean[:, None, None])\n",
    "\n",
    "    img_ = img_.transpose(0, 2).transpose(0, 1)  # C*H*W --> H*W*C\n",
    "    if 'ToTensor' in str(transform_train):\n",
    "        img_ = np.array(img_) * 255\n",
    "\n",
    "    if img_.shape[2] == 3:\n",
    "        img_ = Image.fromarray(img_.astype('uint8')).convert('RGB')\n",
    "    elif img_.shape[2] == 1:\n",
    "        img_ = Image.fromarray(img_.astype('uint8').squeeze())\n",
    "    else:\n",
    "        raise Exception(\"Invalid img shape, expected 1 or 3 in axis 2, but got {}!\".format(img_.shape[2]) )\n",
    "\n",
    "    return img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     3,
     11
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 1.9608e-01, 2.1176e-01,  ..., 3.7647e-01,\n",
      "          3.3333e-01, 2.7843e-01],\n",
      "         [0.0000e+00, 3.6471e-01, 4.0784e-01,  ..., 2.7451e-01,\n",
      "          2.8235e-01, 3.0196e-01],\n",
      "         [0.0000e+00, 4.7843e-01, 4.9020e-01,  ..., 3.2941e-01,\n",
      "          3.2941e-01, 3.2549e-01]],\n",
      "\n",
      "        [[2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         [2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         [2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         ...,\n",
      "         [2.9802e-08, 2.3137e-01, 2.2353e-01,  ..., 4.3137e-01,\n",
      "          3.8431e-01, 3.2941e-01],\n",
      "         [2.9802e-08, 4.0000e-01, 4.2745e-01,  ..., 3.4118e-01,\n",
      "          3.4118e-01, 3.6078e-01],\n",
      "         [2.9802e-08, 5.1373e-01, 5.2549e-01,  ..., 4.0000e-01,\n",
      "          3.9608e-01, 3.9216e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 1.6471e-01, 1.6471e-01,  ..., 3.4902e-01,\n",
      "          2.9412e-01, 2.4314e-01],\n",
      "         [0.0000e+00, 3.3333e-01, 3.6471e-01,  ..., 2.5490e-01,\n",
      "          2.5098e-01, 2.7059e-01],\n",
      "         [0.0000e+00, 4.5098e-01, 4.5882e-01,  ..., 3.1373e-01,\n",
      "          3.0196e-01, 3.0196e-01]]]) 0\n",
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x29E8791F4E0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29e87649390>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "# 构建MyDataset实例\n",
    "train_dir = os.path.join(BASE_DIR, \"..\", \"Data\", \"cifar-10\",  \"cifar10_train\")\n",
    "train_data = CifarDataset(data_dir=train_dir, transform=train_transform)\n",
    "\n",
    "print(train_data.__len__()) \n",
    "\n",
    "img_tensor, label = train_data.__getitem__(6)\n",
    "\n",
    "img_rgb = transform_invert(img_tensor, train_transform)\n",
    "print(img_tensor, label)\n",
    "print(img_rgb)\n",
    "plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 针对cifar-10的ResNet结构：resnet-20/32/44/56/110/1202\n",
    "\n",
    "参考自：   \n",
    "https://github.com/akamaster/pytorch_resnet_cifar10   \n",
    "https://github.com/TingsongYu/ghostnet_cifar10/tree/master/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     32,
     71,
     77
    ]
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        \n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "    \n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "def resnet32():\n",
    "    return ResNet(BasicBlock, [5, 5, 5])\n",
    "\n",
    "def resnet44():\n",
    "    return ResNet(BasicBlock, [7, 7, 7])\n",
    "\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])\n",
    "\n",
    "def resnet110():\n",
    "    return ResNet(BasicBlock, [18, 18, 18])\n",
    "\n",
    "def resnet1202():\n",
    "    return ResNet(BasicBlock, [200, 200, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): LambdaLayer()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): LambdaLayer()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnet_20_model = resnet20() # resnet32 56 110 1202\n",
    "print(resnet_20_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 模型训练器  ModelTrainer\n",
    "\n",
    "定义模型训练类，用于完成模型前向，反向传播，并记录训练loss，accuracy等指标  \n",
    "\n",
    "目的是简化主代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     4,
     43
    ]
   },
   "outputs": [],
   "source": [
    "class ModelTrainer(object):\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def train(data_loader, model, loss_f, optimizer, epoch_id, device, max_epoch):\n",
    "        model.train()\n",
    "\n",
    "        conf_mat = np.zeros((10, 10))   # 混淆矩阵，用于绘图，且计算accuracy，precision，recall等指标很方便\n",
    "        loss_sigma = []\n",
    "\n",
    "        for i, data in enumerate(data_loader):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_f(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 统计预测信息\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # 统计混淆矩阵\n",
    "            for j in range(len(labels)):\n",
    "                cate_i = labels[j].cpu().numpy()\n",
    "                pre_i = predicted[j].cpu().numpy()\n",
    "                conf_mat[cate_i, pre_i] += 1.\n",
    "\n",
    "            # 统计loss\n",
    "            loss_sigma.append(loss.item())                  # 记录每个iterations的loss，待会取均值就得到epochs的loss\n",
    "            acc_avg = conf_mat.trace() / conf_mat.sum()     # 利用混淆矩阵求取accuracy， 矩阵的迹 除以 总元素 \n",
    "\n",
    "            # 每10个iteration 打印一次训练信息，loss为10个iteration的平均\n",
    "            if i % 50 == 50 - 1:\n",
    "                print(\"Training: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                    epoch_id + 1, max_epoch, i + 1, len(data_loader), np.mean(loss_sigma), acc_avg))\n",
    "\n",
    "        return np.mean(loss_sigma), acc_avg, conf_mat\n",
    "\n",
    "    @staticmethod\n",
    "    def valid(data_loader, model, loss_f, device):\n",
    "        model.eval()\n",
    "\n",
    "        conf_mat = np.zeros((10, 10))\n",
    "        loss_sigma = []\n",
    "\n",
    "        for i, data in enumerate(data_loader):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_f(outputs, labels)\n",
    "\n",
    "            # 统计预测信息\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # 统计混淆矩阵\n",
    "            for j in range(len(labels)):\n",
    "                cate_i = labels[j].cpu().numpy()\n",
    "                pre_i = predicted[j].cpu().numpy()\n",
    "                conf_mat[cate_i, pre_i] += 1.\n",
    "\n",
    "            # 统计loss\n",
    "            loss_sigma.append(loss.item())\n",
    "\n",
    "        acc_avg = conf_mat.trace() / conf_mat.sum()\n",
    "\n",
    "        return np.mean(loss_sigma), acc_avg, conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.混淆矩阵概念\n",
    "混淆矩阵(Confusion Matrix)常用来观察分类结果，其是一个N\\*N的方阵，N表示类别数。 \n",
    "\n",
    "混淆矩阵的行表示真实类别，列表示预测类别。例如，猫狗的二分类问题，有猫的图像10张，狗的图像30张，模型对这40张图片进行预测，得到的混淆矩阵为\n",
    "\n",
    "| 类别|  阿猫   | 阿狗  |\n",
    "|----|  ----  | ----  |\n",
    "|阿猫 | 7  | 3 |\n",
    "|阿狗| 10  | 20 |\n",
    "\n",
    "\n",
    "从第一行中可知道，10张猫的图像中，7张预测为猫，3张预测为狗，猫的召回率(Recall)为7/10 = 70%，   \n",
    "从第二行中可知道，30张狗的图像中，8张预测为猫，22张预测为狗，狗的召回率为20/30 = 66.7%，  \n",
    "从第一列中可知道，预测为猫的17张图像中，有7张是真正的猫，猫的精确度(Precision)为7 / 17 = 41.17%   \n",
    "从第二列中可知道，预测为狗的23张图像中，有20张是真正的狗，狗的精确度(Precision)为20 / 23 = 86.96%  \n",
    "\n",
    "模型的准确率(Accuracy)为  (7+20) / 40 = 67.5%   \n",
    "\n",
    "可以发现通过混淆矩阵可以清晰的看出网络模型的分类情况，若再结合上颜色可视化，可方便的看出模型的分类偏好。  \n",
    "\n",
    "\n",
    "<img src=\"imgs/Confusion_Matrixtrain.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1.4_gpu",
   "language": "python",
   "name": "pytorch_1.4_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
