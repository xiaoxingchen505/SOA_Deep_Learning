{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Alvin\\\\Desktop\\\\sota_net\\\\02VGG\\\\VGG-代码\\\\B_VGG'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "num_classes = 2\n",
    "\n",
    "MAX_EPOCH = 3\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.001\n",
    "log_interval = 1\n",
    "val_interval = 1\n",
    "classes = 2\n",
    "start_epoch = -1\n",
    "lr_decay_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\Alvin\\\\Desktop\\\\sota_net\\\\02VGG\\\\VGG-代码\\\\B_VGG\\\\data\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e345204d3c30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;31m# dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCatDogDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[0mvalid_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCatDogDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"valid\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_transform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e345204d3c30>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_dir, mode, split_n, rng_seed, transform)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrng_seed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrng_seed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_n\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_img_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# data_info存储所有图片路径和标签，在DataLoader中通过index读取样本\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e345204d3c30>\u001b[0m in \u001b[0;36m_get_img_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_img_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mimg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mimg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 获取所有图片文件名\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\Alvin\\\\Desktop\\\\sota_net\\\\02VGG\\\\VGG-代码\\\\B_VGG\\\\data\\\\train'"
     ]
    }
   ],
   "source": [
    "def transform_invert(img_, transform_train):\n",
    "    \"\"\"\n",
    "    将data 进行反transfrom操作\n",
    "    :param img_: tensor\n",
    "    :param transform_train: torchvision.transforms\n",
    "    :return: PIL image\n",
    "    \"\"\"\n",
    "    if 'Normalize' in str(transform_train):\n",
    "        norm_transform = list(filter(lambda x: isinstance(x, transforms.Normalize), transform_train.transforms))\n",
    "        mean = torch.tensor(norm_transform[0].mean, dtype=img_.dtype, device=img_.device)\n",
    "        std = torch.tensor(norm_transform[0].std, dtype=img_.dtype, device=img_.device)\n",
    "        img_.mul_(std[:, None, None]).add_(mean[:, None, None])\n",
    "\n",
    "    img_ = img_.transpose(0, 2).transpose(0, 1)  # C*H*W --> H*W*C\n",
    "    if 'ToTensor' in str(transform_train):\n",
    "        img_ = np.array(img_) * 255\n",
    "\n",
    "    if img_.shape[2] == 3:\n",
    "        img_ = Image.fromarray(img_.astype('uint8')).convert('RGB')\n",
    "    elif img_.shape[2] == 1:\n",
    "        img_ = Image.fromarray(img_.astype('uint8').squeeze())\n",
    "    else:\n",
    "        raise Exception(\"Invalid img shape, expected 1 or 3 in axis 2, but got {}!\".format(img_.shape[2]) )\n",
    "\n",
    "    return img_\n",
    "\n",
    "class CatDogDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode=\"train\", split_n=0.9, rng_seed=620, transform=None):\n",
    "        \"\"\"\n",
    "        rmb面额分类任务的Dataset\n",
    "        :param data_dir: str, 数据集所在路径\n",
    "        :param transform: torch.transform，数据预处理\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.data_dir = data_dir\n",
    "        self.rng_seed = rng_seed\n",
    "        self.split_n = split_n\n",
    "        self.data_info = self._get_img_info()  # data_info存储所有图片路径和标签，在DataLoader中通过index读取样本\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path_img, label = self.data_info[index]\n",
    "        img = Image.open(path_img).convert('RGB')     # 0~255\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)   # 在这里做transform，转为tensor等等\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.data_info) == 0:\n",
    "            raise Exception(\"\\ndata_dir:{} is a empty dir! Please checkout your path to images!\".format(self.data_dir))\n",
    "        return len(self.data_info)\n",
    "\n",
    "    def _get_img_info(self):\n",
    "\n",
    "        img_names = os.listdir(self.data_dir)\n",
    "        img_names = list(filter(lambda x: x.endswith('.jpg'), img_names)) # 获取所有图片文件名\n",
    "\n",
    "        random.seed(self.rng_seed)  # 保证每次随机的顺序一致，避免训练集与测试集有交叉\n",
    "        random.shuffle(img_names)\n",
    "\n",
    "        img_labels = [0 if n.startswith('cat') else 1 for n in img_names]  # 根据文件名获取标签\n",
    "\n",
    "        split_idx = int(len(img_labels) * self.split_n) # 25000* 0.9 = 22500  # 选择划分点 \n",
    "        # split_idx = int(100 * self.split_n)\n",
    "        if self.mode == \"train\":\n",
    "            img_set = img_names[:split_idx]     # 数据集90%训练\n",
    "            label_set = img_labels[:split_idx]\n",
    "        elif self.mode == \"valid\":\n",
    "            img_set = img_names[split_idx:]\n",
    "            label_set = img_labels[split_idx:]\n",
    "        else:\n",
    "            raise Exception(\"self.mode 无法识别，仅支持(train, valid)\")\n",
    "\n",
    "        path_img_set = [os.path.join(self.data_dir, n) for n in img_set]  # 拼接路径\n",
    "        data_info = [(n, l) for n, l in zip(path_img_set, label_set)]     # 制作样本对\n",
    "\n",
    "        return data_info\n",
    "\n",
    "norm_mean = [0.485, 0.456, 0.406]  # imagenet 训练集中统计得来\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256)),      # (256, 256) 区别\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "normalizes = transforms.Normalize(norm_mean, norm_std)\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.TenCrop(224, vertical_flip=False),\n",
    "    transforms.Lambda(lambda crops: torch.stack([normalizes(transforms.ToTensor()(crop)) for crop in crops])),\n",
    "])\n",
    "\n",
    "# dataset\n",
    "# data_dir = os.path.join(BASE_DIR, \"data\", \"train\")\n",
    "# train_data = CatDogDataset(data_dir=data_dir, mode=\"train\", transform=train_transform)\n",
    "# valid_data = CatDogDataset(data_dir=data_dir, mode=\"valid\", transform=valid_transform)\n",
    "# print(train_data.__len__()) \n",
    "# print(valid_data.__len__())\n",
    "\n",
    "\n",
    "# fake dataset\n",
    "fake_dir = os.path.join(BASE_DIR,  \"data\", \"fake_dataset\")\n",
    "train_data = CatDogDataset(data_dir=fake_dir, mode=\"train\", transform=train_transform)\n",
    "valid_data = CatDogDataset(data_dir=fake_dir, mode=\"valid\", transform=valid_transform)\n",
    "print(train_data.__len__()) \n",
    "print(valid_data.__len__())\n",
    "\n",
    "\n",
    "img_tensor, label = train_data.__getitem__(1)\n",
    "\n",
    "img_rgb = transform_invert(img_tensor, train_transform)\n",
    "print(img_tensor, label)\n",
    "print(img_rgb)\n",
    "plt.imshow(img_rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-434718d37388>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 构建DataLoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 20  128\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvalid_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 2 4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# 构建DataLoder\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True) # 20  128\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=4) # 2 4\n",
    "\n",
    "print(BATCH_SIZE)\n",
    "print(len(train_loader))\n",
    "print(len(valid_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 构建VGG模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "def get_vgg16(path_state_dict, device, vis_model=False):\n",
    "    \"\"\"\n",
    "    创建模型，加载参数\n",
    "    :param path_state_dict:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model = models.vgg16()\n",
    "    pretrained_state_dict = torch.load(path_state_dict)\n",
    "    model.load_state_dict(pretrained_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    if vis_model:\n",
    "        from torchsummary import summary\n",
    "        summary(model, input_size=(3, 224, 224), device=\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    return model\n",
    "# ============================ step 2/5 模型 ============================\n",
    "path_state_dict = os.path.join(BASE_DIR, \"Data\", \"vgg16-397923af.pth\")\n",
    "vgg16_model = get_vgg16(path_state_dict, device, False)\n",
    "\n",
    "num_ftrs = vgg16_model.classifier._modules[\"6\"].in_features\n",
    "vgg16_model.classifier._modules[\"6\"] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "print(num_ftrs)\n",
    "vgg16_model.to(device)\n",
    "print(vgg16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_model.features[0]  # vgg16网络第一层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 3, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_model.features[0].weight.shape # 64个3*3*3的卷积核"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "<torch.optim.lr_scheduler.StepLR object at 0x0000025AD2590358>\n"
     ]
    }
   ],
   "source": [
    "# ============================ step 3/5 损失函数 ============================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# ============================ step 4/5 优化器 ============================\n",
    "# 冻结卷积层\n",
    "flag = 0\n",
    "# flag = 1\n",
    "if flag:\n",
    "    fc_params_id = list(map(id, vgg16_model.classifier.parameters()))  # 返回的是parameters的 内存地址\n",
    "    base_params = filter(lambda p: id(p) not in fc_params_id, alexnet_model.parameters())\n",
    "    optimizer = optim.SGD([\n",
    "        {'params': base_params, 'lr': LR * 0.1},  # 0\n",
    "        {'params': vgg16_model.classifier.parameters(), 'lr': LR}], momentum=0.9)\n",
    "\n",
    "else:\n",
    "    optimizer = optim.SGD(vgg16_model.parameters(), lr=LR, momentum=0.9)  # 选择优化器\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=0.1)  # 设置学习率下降策略\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(patience=5)\n",
    "\n",
    "print(optimizer)\n",
    "print(scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 迭代训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.3242,  1.3070,  1.3242,  ...,  1.2728,  1.2728,  1.2557],\n",
      "          [ 1.3242,  1.3070,  1.2899,  ...,  1.2728,  1.2728,  1.2557],\n",
      "          [ 1.3755,  1.3584,  1.3242,  ...,  1.3070,  1.3070,  1.3070],\n",
      "          ...,\n",
      "          [-1.5528, -1.5699, -1.5357,  ..., -1.5528, -1.6042, -1.6213],\n",
      "          [-1.5185, -1.5357, -1.5185,  ..., -1.5870, -1.6042, -1.6042],\n",
      "          [-1.5185, -1.5699, -1.5870,  ..., -1.6042, -1.6042, -1.5870]],\n",
      "\n",
      "         [[ 1.2381,  1.2206,  1.2206,  ...,  1.1506,  1.1506,  1.1331],\n",
      "          [ 1.2381,  1.2206,  1.2031,  ...,  1.1506,  1.1506,  1.1331],\n",
      "          [ 1.2906,  1.2731,  1.2206,  ...,  1.1856,  1.1856,  1.1856],\n",
      "          ...,\n",
      "          [-1.4405, -1.4580, -1.4405,  ..., -1.5455, -1.5980, -1.6155],\n",
      "          [-1.3880, -1.3880, -1.4055,  ..., -1.5805, -1.5980, -1.5980],\n",
      "          [-1.3704, -1.4055, -1.4230,  ..., -1.5980, -1.5980, -1.5805]],\n",
      "\n",
      "         [[ 1.2980,  1.2805,  1.3328,  ...,  1.1062,  1.0888,  1.0714],\n",
      "          [ 1.2980,  1.2805,  1.2980,  ...,  1.1062,  1.1062,  1.0888],\n",
      "          [ 1.3502,  1.3328,  1.3328,  ...,  1.1411,  1.1411,  1.1411],\n",
      "          ...,\n",
      "          [-0.5844, -0.6018, -0.5844,  ..., -1.3861, -1.4384, -1.4559],\n",
      "          [-0.5495, -0.5495, -0.5495,  ..., -1.4210, -1.4384, -1.4384],\n",
      "          [-0.5321, -0.5670, -0.5844,  ..., -1.4733, -1.4733, -1.4559]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9817,  1.0502,  1.0331,  ..., -0.4568, -0.6109, -0.7308],\n",
      "          [ 0.3994,  0.4851,  0.4679,  ..., -0.8678, -1.0390, -1.1760],\n",
      "          [-0.0801,  0.0227,  0.0227,  ..., -1.2274, -1.3473, -1.4329],\n",
      "          ...,\n",
      "          [ 2.2147,  2.2147,  2.2147,  ..., -1.1932, -0.5253, -0.4397],\n",
      "          [ 2.2147,  2.2147,  2.2147,  ..., -1.1932, -0.5082, -0.4397],\n",
      "          [ 2.2147,  2.2147,  2.2147,  ..., -1.1760, -0.4739, -0.3883]],\n",
      "\n",
      "         [[ 1.5707,  1.6057,  1.5707,  ..., -0.1800, -0.3375, -0.4601],\n",
      "          [ 0.9580,  1.0105,  0.9755,  ..., -0.6176, -0.8102, -0.9328],\n",
      "          [ 0.4328,  0.5028,  0.5028,  ..., -1.0028, -1.1253, -1.2129],\n",
      "          ...,\n",
      "          [ 2.4286,  2.4286,  2.4286,  ..., -0.4601,  0.2227,  0.2752],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ..., -0.4601,  0.2402,  0.2752],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ..., -0.4426,  0.2752,  0.3277]],\n",
      "\n",
      "         [[ 1.8383,  1.8905,  1.8557,  ...,  0.0779, -0.0615, -0.1661],\n",
      "          [ 1.1934,  1.2457,  1.2108,  ..., -0.3230, -0.4798, -0.6193],\n",
      "          [ 0.6356,  0.7054,  0.6879,  ..., -0.6715, -0.7761, -0.8633],\n",
      "          ...,\n",
      "          [ 2.6226,  2.6226,  2.6226,  ...,  0.0256,  0.6879,  0.7751],\n",
      "          [ 2.6226,  2.6226,  2.6226,  ...,  0.0256,  0.7054,  0.7925],\n",
      "          [ 2.6226,  2.6226,  2.6226,  ...,  0.0431,  0.7402,  0.8448]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1700,  1.2214,  1.2899,  ...,  0.0741,  0.0912,  0.0912],\n",
      "          [ 1.1872,  1.2043,  1.2214,  ..., -0.4568, -0.2513, -0.4568],\n",
      "          [ 1.2385,  1.1872,  1.2214,  ..., -0.6281, -0.1999, -0.8164],\n",
      "          ...,\n",
      "          [ 0.3994,  0.3309,  0.3994,  ...,  0.8789,  0.6906,  0.5193],\n",
      "          [ 0.4166,  0.2796,  0.2967,  ...,  0.7762,  0.5364,  0.6221],\n",
      "          [ 0.4166,  0.3823,  0.2111,  ...,  0.4851,  0.3138,  0.5707]],\n",
      "\n",
      "         [[ 1.0630,  1.1155,  1.1856,  ...,  0.2227,  0.2227,  0.2402],\n",
      "          [ 1.0805,  1.0980,  1.1155,  ..., -0.3375, -0.1275, -0.3025],\n",
      "          [ 1.1331,  1.0805,  1.1155,  ..., -0.5126, -0.0749, -0.6702],\n",
      "          ...,\n",
      "          [ 0.4328,  0.4153,  0.5028,  ...,  0.9230,  0.7304,  0.5553],\n",
      "          [ 0.4328,  0.3102,  0.3627,  ...,  0.8179,  0.5728,  0.6604],\n",
      "          [ 0.4153,  0.3803,  0.2402,  ...,  0.5203,  0.3452,  0.6078]],\n",
      "\n",
      "         [[ 0.9494,  1.0017,  1.0714,  ...,  0.3916,  0.4265,  0.4614],\n",
      "          [ 0.9668,  0.9842,  1.0017,  ..., -0.1661,  0.0779, -0.0964],\n",
      "          [ 1.0191,  0.9668,  1.0017,  ..., -0.3230,  0.1128, -0.4798],\n",
      "          ...,\n",
      "          [ 0.3219,  0.2871,  0.3742,  ...,  0.8448,  0.6705,  0.4962],\n",
      "          [ 0.3568,  0.2348,  0.2871,  ...,  0.7576,  0.5485,  0.6356],\n",
      "          [ 0.3916,  0.3742,  0.1999,  ...,  0.4614,  0.3219,  0.6008]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.2624,  0.3138,  0.4337,  ...,  1.4612,  1.4440,  1.4269],\n",
      "          [ 0.7419,  0.4337,  0.5707,  ...,  1.4783,  1.4440,  1.4440],\n",
      "          [ 0.7077,  0.5536,  0.7419,  ...,  1.4783,  1.4440,  1.4954],\n",
      "          ...,\n",
      "          [-0.1828, -0.0972,  0.1254,  ...,  0.1254,  0.4166,  0.3994],\n",
      "          [ 0.0056,  0.0569,  0.3481,  ...,  0.1083,  0.4337,  0.3994],\n",
      "          [-0.0801,  0.3138,  0.3138,  ...,  0.0398,  0.3309,  0.1597]],\n",
      "\n",
      "         [[ 0.3627,  0.4153,  0.5378,  ...,  1.6758,  1.6583,  1.6408],\n",
      "          [ 0.8529,  0.5378,  0.6779,  ...,  1.6933,  1.6583,  1.6583],\n",
      "          [ 0.8354,  0.6604,  0.8529,  ...,  1.6933,  1.6583,  1.7108],\n",
      "          ...,\n",
      "          [-0.1625, -0.0749,  0.1527,  ...,  0.2402,  0.5378,  0.5203],\n",
      "          [ 0.0301,  0.0826,  0.3978,  ...,  0.2227,  0.5553,  0.5203],\n",
      "          [-0.0224,  0.3803,  0.3803,  ...,  0.1352,  0.4503,  0.2752]],\n",
      "\n",
      "         [[ 0.7751,  0.8274,  0.9494,  ...,  1.9777,  1.9603,  1.9428],\n",
      "          [ 1.2631,  0.9494,  1.0888,  ...,  1.9951,  1.9603,  1.9603],\n",
      "          [ 1.1934,  1.0365,  1.2282,  ...,  1.9951,  1.9603,  2.0125],\n",
      "          ...,\n",
      "          [-0.0092,  0.1476,  0.4265,  ...,  0.5659,  0.8622,  0.8099],\n",
      "          [ 0.2348,  0.3393,  0.6879,  ...,  0.5485,  0.8622,  0.8099],\n",
      "          [ 0.2173,  0.6705,  0.7228,  ...,  0.4962,  0.7751,  0.5834]]],\n",
      "\n",
      "\n",
      "        [[[-1.3473, -1.3473, -1.3473,  ..., -1.1589, -1.1418, -1.1589],\n",
      "          [-1.2959, -1.2959, -1.2959,  ..., -1.1760, -1.1589, -1.1589],\n",
      "          [-1.3473, -1.3473, -1.3473,  ..., -1.3130, -1.1760, -1.1075],\n",
      "          ...,\n",
      "          [-1.6384, -1.6213, -1.6384,  ..., -1.8610, -1.9295, -1.8953],\n",
      "          [-1.6555, -1.6384, -1.6384,  ..., -1.8610, -1.9295, -1.8953],\n",
      "          [-1.6384, -1.6213, -1.6042,  ..., -1.8610, -1.9467, -1.9124]],\n",
      "\n",
      "         [[-1.3880, -1.3880, -1.3880,  ..., -1.2479, -1.2304, -1.2129],\n",
      "          [-1.3004, -1.3004, -1.3004,  ..., -1.2829, -1.2829, -1.2654],\n",
      "          [-1.3529, -1.3529, -1.3529,  ..., -1.4230, -1.2829, -1.2304],\n",
      "          ...,\n",
      "          [-1.8431, -1.8606, -1.8782,  ..., -1.8606, -1.9307, -1.8782],\n",
      "          [-1.8606, -1.8782, -1.8782,  ..., -1.8606, -1.9307, -1.8782],\n",
      "          [-1.8431, -1.8606, -1.8431,  ..., -1.8606, -1.9482, -1.9132]],\n",
      "\n",
      "         [[-1.3513, -1.3513, -1.3513,  ..., -1.4036, -1.3861, -1.3861],\n",
      "          [-1.3164, -1.3164, -1.3164,  ..., -1.3339, -1.3164, -1.2990],\n",
      "          [-1.4036, -1.4036, -1.4036,  ..., -1.4384, -1.2990, -1.2119],\n",
      "          ...,\n",
      "          [-1.7347, -1.7347, -1.7522,  ..., -1.7347, -1.7870, -1.7173],\n",
      "          [-1.7522, -1.7522, -1.7522,  ..., -1.7347, -1.7870, -1.7173],\n",
      "          [-1.7347, -1.7347, -1.7173,  ..., -1.7347, -1.8044, -1.7522]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1529,  0.9988,  1.1187,  ...,  1.5297,  1.7352,  1.7009],\n",
      "          [ 1.5810,  1.4783,  1.5125,  ...,  1.6324,  1.6667,  1.7009],\n",
      "          [ 1.6153,  1.4954,  1.5125,  ...,  1.8379,  1.7865,  1.8379],\n",
      "          ...,\n",
      "          [-1.4329, -1.3302, -1.2617,  ...,  0.1939,  0.1083,  0.0569],\n",
      "          [-1.4500, -1.3815, -1.3130,  ...,  0.2111,  0.1083,  0.0741],\n",
      "          [-1.4329, -1.3644, -1.3473,  ...,  0.1939,  0.1939,  0.1939]],\n",
      "\n",
      "         [[ 1.3431,  1.1681,  1.2731,  ...,  1.6583,  1.8333,  1.7633],\n",
      "          [ 1.7808,  1.6583,  1.6758,  ...,  1.8333,  1.8333,  1.8333],\n",
      "          [ 1.8158,  1.6933,  1.6933,  ...,  2.0434,  1.9559,  2.0084],\n",
      "          ...,\n",
      "          [-1.2654, -1.1429, -1.0378,  ..., -0.6176, -0.2675, -0.0574],\n",
      "          [-1.3354, -1.2479, -1.1604,  ..., -0.4776, -0.2150, -0.0049],\n",
      "          [-1.3880, -1.3179, -1.2829,  ..., -0.3025, -0.0399,  0.1352]],\n",
      "\n",
      "         [[ 1.5420,  1.3677,  1.4548,  ...,  1.6117,  1.8905,  1.8905],\n",
      "          [ 1.9777,  1.8557,  1.8557,  ...,  1.7337,  1.8383,  1.9254],\n",
      "          [ 1.9951,  1.8905,  1.9080,  ...,  1.9777,  1.9951,  2.0997],\n",
      "          ...,\n",
      "          [-0.5495, -0.4798, -0.4101,  ..., -0.2358,  0.1651,  0.4091],\n",
      "          [-0.6193, -0.5670, -0.5147,  ..., -0.0615,  0.2696,  0.5136],\n",
      "          [-0.6541, -0.6018, -0.6018,  ...,  0.1476,  0.4962,  0.7228]]]]) torch.Size([18, 3, 224, 224]) tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0]) torch.Size([18])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 6.00 GiB total capacity; 649.03 MiB already allocated; 3.69 GiB free; 656.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-2019f21c4c2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# 3. update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch_1.4_gpu\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch_1.4_gpu\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 6.00 GiB total capacity; 649.03 MiB already allocated; 3.69 GiB free; 656.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# ============================ step 5/5 训练 ============================\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    vgg16_model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        # 1. forward\n",
    "        inputs, labels = data\n",
    "        print(inputs, inputs.shape, labels, labels.shape)\n",
    "\n",
    "        fake_inputs = torch.randn((1,3, 224, 224), device=device)\n",
    "        fake_labels = torch.ones((1,), dtype=torch.int64, device=device)\n",
    "        \n",
    "        outputs = vgg16_model(fake_inputs)\n",
    "\n",
    "        # 2. backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, fake_labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # 3. update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(outputs)\n",
    "        print(loss.item())\n",
    "        break\n",
    "        \n",
    "    scheduler.step()  # 更新学习率\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
